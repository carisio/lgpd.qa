{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1530be-1625-4ce8-9a87-2db19345b536",
   "metadata": {},
   "source": [
    "# Caderno 2. Testes com embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699d7711-54f2-400d-8ccb-4ddde09b0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import faiss\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e147c1a-82f3-4ddb-ae33-cb2340bcbd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "KEY OpenAI ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_KEY = getpass(\"KEY OpenAI\")\n",
    "NOME_MODELO_EMB_OPENAI = \"text-embedding-3-large\"\n",
    "DIM_MODELO_EMB_OPENAI = 3072\n",
    "\n",
    "# Modelos disponíveis\n",
    "MODELOS_EMB_NOME_E_DIM_EMB = [(NOME_MODELO_EMB_OPENAI, DIM_MODELO_EMB_OPENAI)]\n",
    "\n",
    "# Modelos para gerar. A ideia é que, uma vez que já foi gerado, pode tirar daqui. Daí ele não precisa carregar o arquivo e ver se está lá\n",
    "# MODELOS_EMB_PARA_GERAR = [(NOME_MODELO_EMB_OPENAI, DIM_MODELO_EMB_OPENAI)]\n",
    "MODELOS_EMB_PARA_GERAR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c512e2f3-2dda-4aeb-876c-908a75a0449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVO_EMBEDDINGS_CHUNKS = 'outputs/2 - embeddings/embeddings_chunks.h5'\n",
    "ARQUIVO_EMBEDDINGS_QUESTOES = 'outputs/2 - embeddings/embeddings_questoes.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6ce12-67ef-46f3-95df-3d027cdac48e",
   "metadata": {},
   "source": [
    "# 1. Carregar as bases de dados de chunks e de questões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23eb6ead-8ef3-4174-934d-d728faeed2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "chunks_pesquisa = load_jsonl('inputs/chunks_pesquisa.jsonl')\n",
    "questoes = load_jsonl('inputs/questoes.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec4955-ab5e-4fdb-a845-cd0fdcf272cd",
   "metadata": {},
   "source": [
    "## 1.1 Lista de urns/texto dos chunks e dos ids/enunciados das questões."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f189c430-0950-4c03-9981-74a6524c2d7b",
   "metadata": {},
   "source": [
    "Separa as URN/TEXTO dos chunks e as ID/ENUNCIADO das questões.\n",
    "\n",
    "Isso é necessário porque os embeddings são salvos em listas pareadas no H5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd43302-a14a-4b93-895b-b7792bff8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "urn_chunks = [c['URN'] for c in chunks_pesquisa]\n",
    "texto_chunks = [c['TEXTO'] for c in chunks_pesquisa]\n",
    "\n",
    "id_questoes = [q['ID_QUESTAO'] for q in questoes]\n",
    "enunciado_questoes = [q['ENUNCIADO_COM_ALTERNATIVAS'] for q in questoes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a85c04-7312-4a55-adf7-ebd5caf57596",
   "metadata": {},
   "source": [
    "## 1.2 Gera qrels no formato esperado considerando todos os chunks e no nível de artigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07932e3f-d6f1-469a-b9ad-7a1d6bf068c4",
   "metadata": {},
   "source": [
    "O código para gerar as métricas considera um qrels no dataframe pandas. Gera o qrels no formato esperado pela ferramenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944867d8-47ed-4f1f-9db9-696eb9e43d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um qrels no formato esperado\n",
    "id_questao = []\n",
    "urn_chunk = []\n",
    "score = []\n",
    "rank = []\n",
    "for q in questoes:\n",
    "    total_docs = len(q['URN_FUNDAMENTACAO'])\n",
    "    id_questao += [q['ID_QUESTAO']] * total_docs\n",
    "    urn_chunk += q['URN_FUNDAMENTACAO']\n",
    "    score += [1] * total_docs\n",
    "    rank += list(range(1, total_docs+1))\n",
    "\n",
    "qrels_todos_chunks = pd.DataFrame({\n",
    "    \"QUERY_KEY\": id_questao,\n",
    "    \"DOC_KEY\": urn_chunk,\n",
    "    \"SCORE\": score,\n",
    "    \"RANK\": rank\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917a323-9a79-41c9-a161-9c9127415503",
   "metadata": {},
   "source": [
    "Filtra chunks_pesquisa para isolar apenas os chunks que são artigos completos. Isso apenas será utilizado na criação do índice, em outra seção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dfc0df0-3d6d-44d6-a3e8-0e15614ad0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "urns_chunks_pesquisa_apenas_art = { c['URN'] for c in chunks_pesquisa if c['TIPO'] == 'ART' or c['TIPO'] == 'JUR'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac079f-f68b-4001-b69d-5b806acb1d09",
   "metadata": {},
   "source": [
    "Cria uma segunda lista de questões com o campo URN_FUNDAMENTACAO alterado para considerar apenas o nível de artigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7799824d-8a75-4af6-8ba5-6d431a746542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "\n",
    "padrao = re.compile(r'!art\\d{1,3}')\n",
    "\n",
    "questoes_fund_apenas_art = copy.deepcopy(questoes)\n",
    "\n",
    "for questao in questoes_fund_apenas_art:\n",
    "    nova_fundamentacao = []\n",
    "\n",
    "    for texto in questao.get(\"URN_FUNDAMENTACAO\", []):\n",
    "        match = padrao.search(texto)\n",
    "\n",
    "        if match:\n",
    "            # corta exatamente no final de !artX\n",
    "            nova_fundamentacao.append(texto[:match.end()])\n",
    "        else:\n",
    "            nova_fundamentacao.append(texto)\n",
    "\n",
    "    questao[\"URN_FUNDAMENTACAO\"] = list(set(nova_fundamentacao))\n",
    "\n",
    "# Cria um qrels no formato esperado\n",
    "id_questao = []\n",
    "urn_chunk = []\n",
    "score = []\n",
    "rank = []\n",
    "for q in questoes_fund_apenas_art:\n",
    "    total_docs = len(q['URN_FUNDAMENTACAO'])\n",
    "    id_questao += [q['ID_QUESTAO']] * total_docs\n",
    "    urn_chunk += q['URN_FUNDAMENTACAO']\n",
    "    score += [1] * total_docs\n",
    "    rank += list(range(1, total_docs+1))\n",
    "\n",
    "qrels_apenas_art = pd.DataFrame({\n",
    "    \"QUERY_KEY\": id_questao,\n",
    "    \"DOC_KEY\": urn_chunk,\n",
    "    \"SCORE\": score,\n",
    "    \"RANK\": rank\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb48023-41de-4ada-ac1b-e86c6bcbc01b",
   "metadata": {},
   "source": [
    "## 1.3 Mapa de URN -> CHUNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3b5382-f90c-4e3a-a395-6ecf293f64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_urn_chunk = { c['URN']: c for c in chunks_pesquisa}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8263-c777-4181-95ab-c8d38fe0ed8c",
   "metadata": {},
   "source": [
    "# 2. Criar as estruturas em arquivos H5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fedf22-4d63-475e-98e4-d21a0b467246",
   "metadata": {},
   "source": [
    "Garante que o arquivo existe e que tem os datasets nele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b07a6e-25fd-421e-9022-a2ad14fc7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_estrutura_embeddings(arquivo, nome_id, lista_id):\n",
    "    with h5py.File(arquivo, \"a\") as f:\n",
    "        chunk_size=128\n",
    "        n = len(lista_id)\n",
    "        \n",
    "        if nome_id not in f:\n",
    "            # Quando criar o dataset para as URNS/ID, já cria ele preenchido com todas elas\n",
    "            f.create_dataset(\n",
    "                nome_id,\n",
    "                data=np.array(lista_id, dtype=\"S\"),  # grava tudo de uma vez\n",
    "                maxshape=(None,),\n",
    "                dtype=h5py.string_dtype(encoding=\"utf-8\"),\n",
    "                chunks=True\n",
    "            )\n",
    "    \n",
    "        for nome_modelo, dim_modelo in MODELOS_EMB_NOME_E_DIM_EMB:\n",
    "            if nome_modelo not in f:\n",
    "                # Quando criar o dataset com os embeddings do modelo, cria preenchido com nan\n",
    "                ds = f.create_dataset(\n",
    "                    nome_modelo,\n",
    "                    shape=(n, dim_modelo),\n",
    "                    maxshape=(n, dim_modelo),\n",
    "                    dtype=np.float16,\n",
    "                    compression=\"gzip\",\n",
    "                    chunks=(chunk_size, dim_modelo)\n",
    "                )\n",
    "                ds[:] = np.nan\n",
    "\n",
    "criar_estrutura_embeddings(ARQUIVO_EMBEDDINGS_CHUNKS, 'urn', urn_chunks)\n",
    "criar_estrutura_embeddings(ARQUIVO_EMBEDDINGS_QUESTOES, 'id', id_questoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76b869-ce9b-4c3d-9293-650a72f861c1",
   "metadata": {},
   "source": [
    "Funções auxiliares para saber se já existe embedding associado e para atualizar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a3a3a5a-2967-44bb-b5de-146d0c2888ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def existe_embedding(arquivo, nome_modelo, idx):\n",
    "    with h5py.File(arquivo, \"a\") as f:\n",
    "        ds = f[nome_modelo]\n",
    "\n",
    "        return not np.isnan(ds[idx, 0])\n",
    "    \n",
    "def atualizar_embedding(arquivo, nome_modelo, idx, embedding):\n",
    "    with h5py.File(arquivo, \"a\") as f:\n",
    "        ds = f[nome_modelo]\n",
    "\n",
    "        ds[idx] = np.asarray(embedding, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6abca-056a-4c75-93dc-aeae4abb8f93",
   "metadata": {},
   "source": [
    "# 3. Cria embeddings para o campo TEXTO (chunks) e para o campo ENUNCIADO_COM_ALTERNATIVAS (questões)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c236b9b-62e3-4ab4-86e4-b731b2574099",
   "metadata": {},
   "source": [
    "Funções auxiliares para geração de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a83e7e2-4295-4b00-a69f-a5f31e84cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_emb_com_retry(id, texto, nome_modelo, func_get_emb):\n",
    "    try:\n",
    "        return func_get_emb(nome_modelo, texto)\n",
    "    except Exception as e:\n",
    "        tqdm.write(f'id: {id}. Chunk muito grande. Reduzindo em 20%\\n{e.message}')\n",
    "        # Extrai, da mensagem de erro, o total de tokens requisitados e diminui o texto proporcionalmente.\n",
    "        # No caso da openai, eles aceitam 8192 de entrada.\n",
    "        # Na hora de diminuir, garante que vai diminuir pelo menos 20% do texto de entrada\n",
    "        reduzir_para = int(len(texto)*.8)\n",
    "        if nome_modelo == NOME_MODELO_EMB_OPENAI:\n",
    "            match = re.search(r'requested\\s+(\\d+)\\s+tokens', e.message)\n",
    "             # Se não achou a mensagem, considera 8192 para reduzir em 20%\n",
    "            total_token_requisitados = int(match.group(1)) if match else 8192\n",
    "            reduzir_para = int(min(8192/total_token_requisitados, 0.8) * len(texto))\n",
    "            \n",
    "        return extrai_emb_com_retry(id, texto[:reduzir_para], nome_modelo, func_get_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946dc23-2e2e-4af9-8456-bb10aa6c56d7",
   "metadata": {},
   "source": [
    "Função para extrair embeddings usando o modelo da openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff91428-56da-4a45-bfe0-d64c60f3303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_openai = OpenAI(api_key=OPENAI_KEY, base_url=None)\n",
    "def extrair_embeddings_openai(nome_modelo, texto):\n",
    "   return client_openai.embeddings.create(input = [texto], model=nome_modelo).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fb6c1-e035-4e48-b184-a41383d0f675",
   "metadata": {},
   "source": [
    "Agora gera os embeddings dos chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50682a2c-2da0-49c5-8769-815b62464996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 6932/6932 [00:00<00:00, 1668479.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Varre todos os urns\n",
    "for idx, urn in enumerate(tqdm(urn_chunks)):\n",
    "    texto = texto_chunks[idx]\n",
    "\n",
    "    for nome_modelo, _ in MODELOS_EMB_PARA_GERAR:\n",
    "        if not existe_embedding(ARQUIVO_EMBEDDINGS_CHUNKS, nome_modelo, idx):\n",
    "            # TODO: Depois generalizar isso daqui para não ficar tendo que fazer if com o nome dos modelos\n",
    "            if nome_modelo == NOME_MODELO_EMB_OPENAI:\n",
    "                emb_gerado = extrai_emb_com_retry(urn, texto, nome_modelo, extrair_embeddings_openai)\n",
    "                atualizar_embedding(ARQUIVO_EMBEDDINGS_CHUNKS, nome_modelo, idx, emb_gerado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aece113-b7ef-4247-9b39-e7ca32ba1ce3",
   "metadata": {},
   "source": [
    "Gera os embeddings dos enunciados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c30b27-54e4-4e83-b89b-4aee7f85ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Varre todos os enunciados das questões\n",
    "for idx, id in enumerate(tqdm(id_questoes)):\n",
    "    texto = enunciado_questoes[idx]\n",
    "\n",
    "    for nome_modelo, _ in MODELOS_EMB_PARA_GERAR:\n",
    "        if not existe_embedding(ARQUIVO_EMBEDDINGS_QUESTOES, nome_modelo, idx):\n",
    "            # TODO: Depois generalizar isso daqui para não ficar tendo que fazer if com o nome dos modelos\n",
    "            if nome_modelo == NOME_MODELO_EMB_OPENAI:\n",
    "                emb_gerado = extrai_emb_com_retry(id, texto, nome_modelo, extrair_embeddings_openai)\n",
    "                atualizar_embedding(ARQUIVO_EMBEDDINGS_QUESTOES, nome_modelo, idx, emb_gerado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fbff6-6326-4c01-8890-17f8e3568756",
   "metadata": {},
   "source": [
    "# 4. Pesquisa semântica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb890f-f7fe-4772-8118-24f184cdeec2",
   "metadata": {},
   "source": [
    "Função auxiliar para, considerando uma lista ordenada de urns, selecionar as n primeiras urns considerando a regra de que nessa lista uma urn não pode englobar outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d1a4ed8-3063-40ca-b015-04cb03c2c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para, dado uma lista de urns, selecionar as primeiras n urns.\n",
    "# Para isso, é feita uma consideração de que uma urn pode englobar outra (vendo os atributos INICIO e FIM da urn).\n",
    "# A lógica implementada é:\n",
    "# 1. Começa a lista de resultados como vazia\n",
    "# 2. Pega o próximo elemento de urns (a lista de chunks). Chamando NOVO\n",
    "# 3. Antes de inserir na lista de resultados, verifica:\n",
    "# 3.1. Se NOVO engloba algum elemento que está na lista, NOVO assume a posição do outro elemento, que sai da lista\n",
    "# 3.2. Se NOVO é englobado por algum elemento que já está na lista, NOVO não entra na lista\n",
    "# 3.3. Caso 3.1 ou 3.2 não ocorra, NOVO entra na lista\n",
    "#\n",
    "# Com isso, as n urns retornadas serão as que mais englobam contexto.\n",
    "# Assim, é importante selecionar sempre o número que for usar no RAG.\n",
    "# Por exemplo, se for utilizar só 3, o ideal é informar n=3. Se for utilizar só 5, informar n=5.\n",
    "# Da forma como foi feita a implementação, informar n=5 e pegar os 3 primeiros resultados pode dar um resultado diferente \n",
    "# do que apenas informar n=3. Por exemplo, suponha que o resultado da lista é [art1_cpt_inc1, art2, art3, art1, art4, art6].\n",
    "# Se usamos n=3, o resultado deverá ser [art1_cpt_inc1, art2, art3].\n",
    "# No entanto, se usarmos n=5, o resultado deverá ser [art1, art2, art3, art4, art6], pois art1_cpt_inc1 tomou o lugar de art1.  \n",
    "def seleciona_n_urns(urns, n):\n",
    "    resultados = []\n",
    "\n",
    "    for urn in urns:\n",
    "        if len(resultados) >= n:\n",
    "            break\n",
    "\n",
    "        chunk_novo = mapa_urn_chunk[urn]\n",
    "        ini_novo = chunk_novo['INICIO']\n",
    "        fim_novo = chunk_novo['FIM']\n",
    "\n",
    "        # EXCEÇÃO: INICIO == -1 → insere direto\n",
    "        if ini_novo == -1:\n",
    "            resultados.append(urn)\n",
    "            continue\n",
    "            \n",
    "        indices_engloba = []\n",
    "        descartar = False\n",
    "\n",
    "        for i, urn_existente in enumerate(resultados):\n",
    "            chunk_exist = mapa_urn_chunk[urn_existente]\n",
    "            ini_exist = chunk_exist['INICIO']\n",
    "            fim_exist = chunk_exist['FIM']\n",
    "\n",
    "            # Caso 1: existente engloba novo → descarta\n",
    "            if ini_exist <= ini_novo and fim_exist >= fim_novo:\n",
    "                descartar = True\n",
    "                break\n",
    "\n",
    "            # Caso 2: novo engloba existente → marca para remoção\n",
    "            if ini_novo <= ini_exist and fim_novo >= fim_exist:\n",
    "                indices_engloba.append(i)\n",
    "\n",
    "        if descartar:\n",
    "            continue\n",
    "\n",
    "        if indices_engloba:\n",
    "            # remove do fim para o começo para não bagunçar índices\n",
    "            for i in reversed(indices_engloba):\n",
    "                del resultados[i]\n",
    "\n",
    "            # insere na posição do primeiro removido\n",
    "            resultados.insert(indices_engloba[0], urn)\n",
    "        else:\n",
    "            resultados.append(urn)\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c304dce-7323-4172-8e5e-5647f6691d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urn:lex:br:federal:constituicao:1988-10-05;1988!art1',\n",
       " 'urn:lex:br:federal:constituicao:1988-10-05;1988!art2_cpt',\n",
       " 'urn:lex:br:federal:constituicao:1988-10-05;1988!art3_cpt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Teste\n",
    "seleciona_n_urns([\n",
    "    \"urn:lex:br:federal:constituicao:1988-10-05;1988!art1_par1u\",\n",
    "    \"urn:lex:br:federal:constituicao:1988-10-05;1988!art1_cpt\",\n",
    "    \"urn:lex:br:federal:constituicao:1988-10-05;1988!art1\",\n",
    "    \"urn:lex:br:federal:constituicao:1988-10-05;1988!art2_cpt\",\n",
    "    \"urn:lex:br:federal:constituicao:1988-10-05;1988!art3_cpt\"\n",
    "    ],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cff97-d33d-43a9-b734-48764a151cba",
   "metadata": {},
   "source": [
    "## 4.2 Pesquisa com o FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232d514-552d-45fd-a600-dac68b580a20",
   "metadata": {},
   "source": [
    "Carregar e normalizar embeddings.\n",
    "\n",
    "Obs.: Os embeddings da OpenAI já são normalizados (mas há uma perda na conversão de f32 pra f16 para salvar no H5), nem precisaria disso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e831a3d-ca00-4cf5-946f-567b2a1388ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_embeddings_faiss(arquivo, coluna_id, nome_modelo):\n",
    "    with h5py.File(arquivo, \"r\") as f:\n",
    "        ids = f[coluna_id][:].astype(str)\n",
    "        emb = f[nome_modelo][:].astype(np.float32)\n",
    "\n",
    "    return ids, emb\n",
    "\n",
    "# A ideia dessa função é, uma vez carregado todos os embeddings, filtrar apenas aqueles\n",
    "# que batem com a máscara. Será usado para indexar apenas os embeddings que se referem a artigos completos\n",
    "def filtro_apenas_art(urn):\n",
    "    return urn in urns_chunks_pesquisa_apenas_art\n",
    "\n",
    "def filtro_seleciona_tudo(_):\n",
    "    return True\n",
    "    \n",
    "def filtrar_embeddings(urns, emb, func_filtro):\n",
    "    mask = np.array([func_filtro(i) for i in urns], dtype=bool)\n",
    "\n",
    "    return urns[mask], emb[mask]\n",
    "\n",
    "def normalizar_embeddings(x):\n",
    "    faiss.normalize_L2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f681138-1c84-43e8-83b4-b7f2e0336bfc",
   "metadata": {},
   "source": [
    "Criar o índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54b14efc-8630-448d-8a83-09838264771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice criado com 6932 vetores\n",
      "Índice criado com 1078 vetores\n"
     ]
    }
   ],
   "source": [
    "def criar_mapa_indice_chunks(arquivo, filtro_seleciona_id):\n",
    "    mapa_indice_faiss = {}\n",
    "    \n",
    "    for nome_modelo, _ in MODELOS_EMB_NOME_E_DIM_EMB:\n",
    "        urns, emb_chunks = carregar_embeddings_faiss(arquivo, 'urn', nome_modelo)\n",
    "        urns, emb_chunks = filtrar_embeddings(urns, emb_chunks, filtro_seleciona_id)\n",
    "                                           \n",
    "        emb_chunks = normalizar_embeddings(emb_chunks)\n",
    "        dim = emb_chunks.shape[1]\n",
    "        \n",
    "        index_faiss = faiss.IndexFlatL2(dim)\n",
    "        index_faiss.add(emb_chunks)\n",
    "    \n",
    "        mapa_indice_faiss[nome_modelo] = index_faiss\n",
    "        print(f\"Índice criado com {index_faiss.ntotal} vetores\")\n",
    "    \n",
    "    return urns, mapa_indice_faiss\n",
    "\n",
    "urns_todos_chunks, mapa_indice_faiss_todos_chunks_por_modelo = criar_mapa_indice_chunks(ARQUIVO_EMBEDDINGS_CHUNKS, filtro_seleciona_tudo)\n",
    "urns_apenas_art, mapa_indice_faiss_apenas_art_por_modelo = criar_mapa_indice_chunks(ARQUIVO_EMBEDDINGS_CHUNKS, filtro_apenas_art)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984032d-3c3c-4f70-8c69-93dc07147fec",
   "metadata": {},
   "source": [
    "Função de busca no índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "635ccf91-6415-422c-8259-264c7545443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapa embeddings da questão por modelo\n",
    "mapa_emb_questao_por_modelo = {}\n",
    "for nome_modelo, _ in MODELOS_EMB_NOME_E_DIM_EMB:\n",
    "    ids_q, emb_q = carregar_embeddings_faiss(ARQUIVO_EMBEDDINGS_QUESTOES, 'id', nome_modelo)\n",
    "    emb_q = normalizar_embeddings(emb_q)\n",
    "    mapa_emb_questao_por_modelo[nome_modelo] = emb_q\n",
    "\n",
    "# Mapa id da questão por índice. Considera que todos os embeddings foram criados sequencialmente (foi feito assim mesmo)\n",
    "mapa_id_questao = {id_: i for i, id_ in enumerate(ids_q)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "311bc033-b4eb-4309-90b1-dd7703a58b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_proximos_faiss(id_questao, nome_modelo, urns, mapa_indices, n_chunks=20):\n",
    "    idx_q = mapa_id_questao[id_questao]\n",
    "    query =  mapa_emb_questao_por_modelo[nome_modelo][idx_q:idx_q+1]  # shape (1, dim)\n",
    "    \n",
    "    distancias, indices = mapa_indices[nome_modelo].search(query, n_chunks)\n",
    "\n",
    "    resultados = []\n",
    "    for rank, (i, d) in enumerate(zip(indices[0], distancias[0])):\n",
    "        resultados.append({\n",
    "            \"rank\": rank + 1,\n",
    "            \"urn\": urns[i],\n",
    "            \"distancia_l2\": float(d),\n",
    "            \"similaridade_cosine_aprox\": 1 - d / 2\n",
    "        })\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181283e0-9136-48d2-9a58-c49ff87d9b0d",
   "metadata": {},
   "source": [
    "Alguns testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b46f250-4add-4421-8170-e1f4a01c73e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'urn': 'tema_stf_483',\n",
       "  'distancia_l2': 0.8046747446060181,\n",
       "  'similaridade_cosine_aprox': 0.597662627696991},\n",
       " {'rank': 2,\n",
       "  'urn': 'urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt',\n",
       "  'distancia_l2': 0.9386035799980164,\n",
       "  'similaridade_cosine_aprox': 0.5306982100009918},\n",
       " {'rank': 3,\n",
       "  'urn': 'urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt_inc4',\n",
       "  'distancia_l2': 0.9555093050003052,\n",
       "  'similaridade_cosine_aprox': 0.5222453474998474}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chunks_proximos_faiss('675', NOME_MODELO_EMB_OPENAI, urns_todos_chunks, mapa_indice_faiss_todos_chunks_por_modelo, n_chunks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af2077f-825f-4222-9908-ec9585de1c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rank': 1,\n",
       "  'urn': 'tema_stf_483',\n",
       "  'distancia_l2': 0.8046747446060181,\n",
       "  'similaridade_cosine_aprox': 0.597662627696991},\n",
       " {'rank': 2,\n",
       "  'urn': 'urn:lex:br:federal:lei:2011-11-18;12527!art34',\n",
       "  'distancia_l2': 0.9825958013534546,\n",
       "  'similaridade_cosine_aprox': 0.5087020993232727},\n",
       " {'rank': 3,\n",
       "  'urn': 'urn:lex:br:autoridade.nacional.protecao.dados;conselho.diretor:resolucao:2024-07-16;18;anexo.1!art5',\n",
       "  'distancia_l2': 0.9892439842224121,\n",
       "  'similaridade_cosine_aprox': 0.505378007888794}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_chunks_proximos_faiss('675', NOME_MODELO_EMB_OPENAI, urns_apenas_art, mapa_indice_faiss_apenas_art_por_modelo, n_chunks=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bac8f1-3a7e-423a-8a42-9c4ab73e1b3a",
   "metadata": {},
   "source": [
    "Calcula as métricas para toda a base de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d231c-1d8c-48f7-93d4-6906a32d8167",
   "metadata": {},
   "source": [
    "# TODO: PRECISA TERMINAR DE IMPLEMENTAR A PARTE DE CHAMAR SELECIONA_N_URNS PARA QUE A PESQUISA USE A LÓGICA DE ENGLOBAR OS CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7832792-a480-4710-bf8f-7013eaac6439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesquisa_semantica(urns_nos_indices, mapa_indice_faiss, n_chunks=20):   \n",
    "    mapa_df_resultado_por_modelo = {}\n",
    "\n",
    "    total_porcentagem = len(MODELOS_EMB_NOME_E_DIM_EMB)*len(questoes)\n",
    "    with tqdm(total=total_porcentagem) as pbar:\n",
    "        for nome_modelo, _ in MODELOS_EMB_NOME_E_DIM_EMB:\n",
    "            col_resultado_id_questao=[]\n",
    "            col_resultado_urn_chunk=[]\n",
    "            col_resultado_rank=[]\n",
    "            \n",
    "            for q in questoes:\n",
    "                id_questao = q['ID_QUESTAO']\n",
    "                resultados_para_id_questao = get_chunks_proximos_faiss(id_questao, nome_modelo, urns_nos_indices, mapa_indice_faiss, n_chunks)\n",
    "                \n",
    "                ids_questao = [id_questao] * len(resultados_para_id_questao)\n",
    "                primeiros_20_urns = [item['urn'] for item in resultados_para_id_questao]\n",
    "                ranking = [item['rank'] for item in resultados_para_id_questao]\n",
    "        \n",
    "                col_resultado_id_questao.extend(ids_questao)\n",
    "                col_resultado_urn_chunk.extend(primeiros_20_urns)\n",
    "                col_resultado_rank.extend(ranking)\n",
    "    \n",
    "                pbar.update(1)\n",
    "        \n",
    "            df_resultados_por_modelo = pd.DataFrame({\n",
    "                \"QUERY_KEY\": col_resultado_id_questao,\n",
    "                \"DOC_KEY\": col_resultado_urn_chunk,\n",
    "                \"RANK\": col_resultado_rank,\n",
    "            })\n",
    "            mapa_df_resultado_por_modelo[nome_modelo] = df_resultados_por_modelo\n",
    "    \n",
    "    return mapa_df_resultado_por_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "193b8e60-cd69-4c00-b4c7-636811a0d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 700/700 [00:03<00:00, 177.71it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<00:00, 2468.18it/s]\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "mapa_resultados_todos_chunks_por_modelo = pesquisa_semantica(urns_todos_chunks, mapa_indice_faiss_todos_chunks_por_modelo, n_chunks=k)\n",
    "mapa_resultados_apenas_art_por_modelo = pesquisa_semantica(urns_apenas_art, mapa_indice_faiss_apenas_art_por_modelo, n_chunks=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f47a887-59e4-43fb-8cb0-db01e62ef66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from metricas import histograma_metricas, boxplot_metricas, metricas\n",
    "\n",
    "mapa_metricas_todos_chunks = {}\n",
    "mapa_metricas_apenas_art = {}\n",
    "\n",
    "for nome_modelo, _ in tqdm(MODELOS_EMB_NOME_E_DIM_EMB):\n",
    "    mapa_metricas_todos_chunks[nome_modelo] = metricas(mapa_resultados_todos_chunks_por_modelo[nome_modelo], qrels_todos_chunks, aproximacao_trec_eval=True, k=[k])\n",
    "    mapa_metricas_apenas_art[nome_modelo] = metricas(mapa_resultados_apenas_art_por_modelo[nome_modelo], qrels_apenas_art, aproximacao_trec_eval=True, k=[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cd8f3b3-e2a3-48a3-8673-3dcefd0783ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################## text-embedding-3-large ##########################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@5</th>\n",
       "      <th>R@5</th>\n",
       "      <th>MRR@5</th>\n",
       "      <th>nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.183429</td>\n",
       "      <td>0.489627</td>\n",
       "      <td>0.455786</td>\n",
       "      <td>0.408552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.153396</td>\n",
       "      <td>0.421263</td>\n",
       "      <td>0.398562</td>\n",
       "      <td>0.361241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.386853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.630930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              P@5         R@5       MRR@5      nDCG@5\n",
       "count  700.000000  700.000000  700.000000  700.000000\n",
       "mean     0.183429    0.489627    0.455786    0.408552\n",
       "std      0.153396    0.421263    0.398562    0.361241\n",
       "min      0.000000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    0.000000    0.000000    0.000000\n",
       "50%      0.200000    0.400000    0.333333    0.386853\n",
       "75%      0.200000    1.000000    1.000000    0.630930\n",
       "max      0.800000    1.000000    1.000000    1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for nome_modelo, _ in MODELOS_EMB_NOME_E_DIM_EMB:\n",
    "    print(f'########################## {nome_modelo} ##########################')\n",
    "    display(mapa_metricas_todos_chunks[nome_modelo].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3892c29c-3651-4bd2-9cba-ca87fd6d9e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################## text-embedding-3-large ##########################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P@5</th>\n",
       "      <th>R@5</th>\n",
       "      <th>MRR@5</th>\n",
       "      <th>nDCG@5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "      <td>700.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.227429</td>\n",
       "      <td>0.771704</td>\n",
       "      <td>0.727738</td>\n",
       "      <td>0.702141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.158468</td>\n",
       "      <td>0.371088</td>\n",
       "      <td>0.388643</td>\n",
       "      <td>0.361167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              P@5         R@5       MRR@5      nDCG@5\n",
       "count  700.000000  700.000000  700.000000  700.000000\n",
       "mean     0.227429    0.771704    0.727738    0.702141\n",
       "std      0.158468    0.371088    0.388643    0.361167\n",
       "min      0.000000    0.000000    0.000000    0.000000\n",
       "25%      0.200000    0.600000    0.500000    0.500000\n",
       "50%      0.200000    1.000000    1.000000    0.881338\n",
       "75%      0.200000    1.000000    1.000000    1.000000\n",
       "max      0.800000    1.000000    1.000000    1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for nome_modelo, _ in MODELOS_EMB_NOME_E_DIM_EMB:\n",
    "    print(f'########################## {nome_modelo} ##########################')\n",
    "    display(mapa_metricas_apenas_art[nome_modelo].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1f011c27-213c-4234-a124-62d75f13fd03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tema_stf_483',\n",
       " 'urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt',\n",
       " 'urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt_inc4',\n",
       " 'urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt_inc2',\n",
       " 'urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt_inc1']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = mapa_resultados_todos_chunks_por_modelo[NOME_MODELO_EMB_OPENAI]\n",
    "df[df.QUERY_KEY == '675'].DOC_KEY.iloc[0:k].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54a23cef-d038-4fcd-9a41-113c0cc28d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tema_stf_483',\n",
       " 'urn:lex:br:federal:lei:2011-11-18;12527!art34',\n",
       " 'urn:lex:br:autoridade.nacional.protecao.dados;conselho.diretor:resolucao:2024-07-16;18;anexo.1!art5',\n",
       " 'urn:lex:br:federal:lei:2018-08-14;13709!art23',\n",
       " 'urn:lex:br:federal:lei:2018-08-14;13709!art26']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = mapa_resultados_apenas_art_por_modelo[NOME_MODELO_EMB_OPENAI]\n",
    "df[df.QUERY_KEY == '675'].DOC_KEY.iloc[0:k].tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
