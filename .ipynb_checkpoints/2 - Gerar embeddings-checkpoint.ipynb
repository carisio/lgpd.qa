{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b1530be-1625-4ce8-9a87-2db19345b536",
   "metadata": {},
   "source": [
    "# Caderno 2. Gerar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699d7711-54f2-400d-8ccb-4ddde09b0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e147c1a-82f3-4ddb-ae33-cb2340bcbd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "KEY OpenAI ········\n"
     ]
    }
   ],
   "source": [
    "OPENAI_KEY = getpass(\"KEY OpenAI\")\n",
    "NOME_MODELO_EMB_OPENAI = \"text-embedding-3-large\"\n",
    "DIM_MODELO_EMB_OPENAI = 3072\n",
    "\n",
    "# Modelos disponíveis\n",
    "MODELOS_EMB_NOME_E_DIM_EMB = [(NOME_MODELO_EMB_OPENAI, DIM_MODELO_EMB_OPENAI)]\n",
    "\n",
    "# Modelos para gerar. A ideia é que, uma vez que já foi gerado, pode tirar daqui. Daí ele não precisa carregar o arquivo e ver se está lá\n",
    "# MODELOS_EMB_PARA_GERAR = [(NOME_MODELO_EMB_OPENAI, DIM_MODELO_EMB_OPENAI)]\n",
    "MODELOS_EMB_PARA_GERAR = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c512e2f3-2dda-4aeb-876c-908a75a0449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARQUIVO_EMBEDDINGS_CHUNKS = 'outputs/2 - embeddings/embeddings_chunks.h5'\n",
    "ARQUIVO_EMBEDDINGS_QUESTOES = 'outputs/2 - embeddings/embeddings_questoes.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6ce12-67ef-46f3-95df-3d027cdac48e",
   "metadata": {},
   "source": [
    "# 1. Carregar as bases de dados de chunks e de questões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23eb6ead-8ef3-4174-934d-d728faeed2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "chunks_pesquisa = load_jsonl('inputs/chunks_pesquisa.jsonl')\n",
    "questoes = load_jsonl('inputs/questoes.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec4955-ab5e-4fdb-a845-cd0fdcf272cd",
   "metadata": {},
   "source": [
    "## 1.1 Lista de urns/texto dos chunks e dos ids/enunciados das questões."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f189c430-0950-4c03-9981-74a6524c2d7b",
   "metadata": {},
   "source": [
    "Separa as URN/TEXTO dos chunks e as ID/ENUNCIADO das questões.\n",
    "\n",
    "Isso é necessário porque os embeddings são salvos em listas pareadas no H5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd43302-a14a-4b93-895b-b7792bff8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "urn_chunks = [c['URN'] for c in chunks_pesquisa]\n",
    "texto_chunks = [c['TEXTO'] for c in chunks_pesquisa]\n",
    "\n",
    "id_questoes = [q['ID_QUESTAO'] for q in questoes]\n",
    "enunciado_questoes = [q['ENUNCIADO_COM_ALTERNATIVAS'] for q in questoes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ad8263-c777-4181-95ab-c8d38fe0ed8c",
   "metadata": {},
   "source": [
    "# 2. Criar as estruturas em arquivos H5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fedf22-4d63-475e-98e4-d21a0b467246",
   "metadata": {},
   "source": [
    "Garante que o arquivo existe e que tem os datasets nele:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4b07a6e-25fd-421e-9022-a2ad14fc7c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_estrutura_embeddings(arquivo, nome_id, lista_id):\n",
    "    with h5py.File(arquivo, \"a\") as f:\n",
    "        chunk_size=128\n",
    "        n = len(lista_id)\n",
    "        \n",
    "        if nome_id not in f:\n",
    "            # Quando criar o dataset para as URNS/ID, já cria ele preenchido com todas elas\n",
    "            f.create_dataset(\n",
    "                nome_id,\n",
    "                data=np.array(lista_id, dtype=\"S\"),  # grava tudo de uma vez\n",
    "                maxshape=(None,),\n",
    "                dtype=h5py.string_dtype(encoding=\"utf-8\"),\n",
    "                chunks=True\n",
    "            )\n",
    "    \n",
    "        for nome_modelo, dim_modelo in MODELOS_EMB_NOME_E_DIM_EMB:\n",
    "            if nome_modelo not in f:\n",
    "                # Quando criar o dataset com os embeddings do modelo, cria preenchido com nan\n",
    "                ds = f.create_dataset(\n",
    "                    nome_modelo,\n",
    "                    shape=(n, dim_modelo),\n",
    "                    maxshape=(n, dim_modelo),\n",
    "                    dtype=np.float16,\n",
    "                    compression=\"gzip\",\n",
    "                    chunks=(chunk_size, dim_modelo)\n",
    "                )\n",
    "                ds[:] = np.nan\n",
    "\n",
    "criar_estrutura_embeddings(ARQUIVO_EMBEDDINGS_CHUNKS, 'urn', urn_chunks)\n",
    "criar_estrutura_embeddings(ARQUIVO_EMBEDDINGS_QUESTOES, 'id', id_questoes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76b869-ce9b-4c3d-9293-650a72f861c1",
   "metadata": {},
   "source": [
    "Funções auxiliares para saber se já existe embedding associado e para atualizar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a3a3a5a-2967-44bb-b5de-146d0c2888ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def existe_embedding(arquivo, nome_modelo, idx):\n",
    "    with h5py.File(arquivo, \"a\") as f:\n",
    "        ds = f[nome_modelo]\n",
    "\n",
    "        return not np.isnan(ds[idx, 0])\n",
    "    \n",
    "def atualizar_embedding(arquivo, nome_modelo, idx, embedding):\n",
    "    with h5py.File(arquivo, \"a\") as f:\n",
    "        ds = f[nome_modelo]\n",
    "\n",
    "        ds[idx] = np.asarray(embedding, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6abca-056a-4c75-93dc-aeae4abb8f93",
   "metadata": {},
   "source": [
    "# 3. Cria embeddings para o campo TEXTO (chunks da base de pesquisa) e para o campo ENUNCIADO_COM_ALTERNATIVAS (questões)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c236b9b-62e3-4ab4-86e4-b731b2574099",
   "metadata": {},
   "source": [
    "Funções auxiliares para geração de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a83e7e2-4295-4b00-a69f-a5f31e84cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_emb_com_retry(id, texto, nome_modelo, func_get_emb):\n",
    "    try:\n",
    "        return func_get_emb(nome_modelo, texto)\n",
    "    except Exception as e:\n",
    "        tqdm.write(f'id: {id}. Chunk muito grande. Reduzindo em 20%\\n{e.message}')\n",
    "        # Extrai, da mensagem de erro, o total de tokens requisitados e diminui o texto proporcionalmente.\n",
    "        # No caso da openai, eles aceitam 8192 de entrada.\n",
    "        # Na hora de diminuir, garante que vai diminuir pelo menos 20% do texto de entrada\n",
    "        reduzir_para = int(len(texto)*.8)\n",
    "        if nome_modelo == NOME_MODELO_EMB_OPENAI:\n",
    "            match = re.search(r'requested\\s+(\\d+)\\s+tokens', e.message)\n",
    "             # Se não achou a mensagem, considera 8192 para reduzir em 20%\n",
    "            total_token_requisitados = int(match.group(1)) if match else 8192\n",
    "            reduzir_para = int(min(8192/total_token_requisitados, 0.8) * len(texto))\n",
    "            \n",
    "        return extrai_emb_com_retry(id, texto[:reduzir_para], nome_modelo, func_get_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4946dc23-2e2e-4af9-8456-bb10aa6c56d7",
   "metadata": {},
   "source": [
    "Função para extrair embeddings para os modelos da openAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ff91428-56da-4a45-bfe0-d64c60f3303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_openai = OpenAI(api_key=OPENAI_KEY, base_url=None)\n",
    "def extrair_embeddings_openai(nome_modelo, texto):\n",
    "   return client_openai.embeddings.create(input = [texto], model=nome_modelo).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fb6c1-e035-4e48-b184-a41383d0f675",
   "metadata": {},
   "source": [
    "Agora gera os embeddings dos chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50682a2c-2da0-49c5-8769-815b62464996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 6932/6932 [00:00<00:00, 1668479.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Varre todos os urns\n",
    "for idx, urn in enumerate(tqdm(urn_chunks)):\n",
    "    texto = texto_chunks[idx]\n",
    "\n",
    "    for nome_modelo, _ in MODELOS_EMB_PARA_GERAR:\n",
    "        if not existe_embedding(ARQUIVO_EMBEDDINGS_CHUNKS, nome_modelo, idx):\n",
    "            \n",
    "            # TODO: Depois generalizar isso daqui para não ficar tendo que fazer if com o nome dos modelos\n",
    "            if nome_modelo == NOME_MODELO_EMB_OPENAI:\n",
    "                emb_gerado = extrai_emb_com_retry(urn, texto, nome_modelo, extrair_embeddings_openai)\n",
    "                atualizar_embedding(ARQUIVO_EMBEDDINGS_CHUNKS, nome_modelo, idx, emb_gerado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aece113-b7ef-4247-9b39-e7ca32ba1ce3",
   "metadata": {},
   "source": [
    "Gera os embeddings dos enunciados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63c30b27-54e4-4e83-b89b-4aee7f85ec80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 700/700 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Varre todos os enunciados das questões\n",
    "for idx, id in enumerate(tqdm(id_questoes)):\n",
    "    texto = enunciado_questoes[idx]\n",
    "\n",
    "    for nome_modelo, _ in MODELOS_EMB_PARA_GERAR:\n",
    "        if not existe_embedding(ARQUIVO_EMBEDDINGS_QUESTOES, nome_modelo, idx):\n",
    "            # TODO: Depois generalizar isso daqui para não ficar tendo que fazer if com o nome dos modelos\n",
    "            if nome_modelo == NOME_MODELO_EMB_OPENAI:\n",
    "                emb_gerado = extrai_emb_com_retry(id, texto, nome_modelo, extrair_embeddings_openai)\n",
    "                atualizar_embedding(ARQUIVO_EMBEDDINGS_QUESTOES, nome_modelo, idx, emb_gerado)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
