{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8788950b-0c86-4377-af5a-7885f9a52a5c",
   "metadata": {},
   "source": [
    "# Caderno 3. Pesquisa chunks relacionados às questões"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d058eee7-ad92-45d1-a6ea-ca5c0deeb399",
   "metadata": {},
   "source": [
    "Esse caderno apenas pesquisa, para cada query, quais são os 500 primeiros resultados.\n",
    "\n",
    "Para os testes nos próximos cadernos, não usaremos os 500 primeiros resultados. Provavelmente usaremos apenas de 0 a 5 resultados. No entanto, como é possível que tenhamos que fazer testes com exemplos pouco relacionados à consulta, vou salvar os 500 primeiros resultados mesmo. Caso sejam irrelevantes, basta desconsiderá-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77444a5d-3bcb-4bc3-8550-a4114f49dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\P_8454\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\P_8454\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\P_8454\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "from bm25 import IndiceInvertido, BM25, tokenizador_pt_remove_html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea1a8e-4bdd-49c9-b198-6631fd554589",
   "metadata": {},
   "source": [
    "Total de registros para buscar e local de salvamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9105238e-4cf9-4cf6-bf0a-7c5578785ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "\n",
    "NOME_ARQUIVO_RESULTADOS_PESQUISAS_TODOS_CHUNKS = 'outputs/3 - resultados_pesquisas/resultados_pesquisas_todos_chunks.pickle.gz'\n",
    "NOME_ARQUIVO_RESULTADOS_PESQUISAS_APENAS_ART = 'outputs/3 - resultados_pesquisas/resultados_pesquisas_apenas_art.pickle.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34de431-0174-409d-b094-9af41407fcbe",
   "metadata": {},
   "source": [
    "Nome dos índices para pesquisa léxica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a705041-44af-4021-b5f8-d0489f824de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME_ARQUIVO_INDICE_BM25_TODOS_CHUNKS = 'outputs/1 - indices_invertidos/indice_bm25_todos_chunks.pickle'\n",
    "NOME_ARQUIVO_INDICE_BM25_APENAS_ART = 'outputs/1 - indices_invertidos/indice_bm25_apenas_art.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dcd77-5bb6-4801-a513-ae727a64400b",
   "metadata": {},
   "source": [
    "Nome dos modelos utilizados para pesquisa semântica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5a14e9-071f-4dea-8046-da9835179a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOME_ARQUIVO_EMBEDDINGS_CHUNKS = 'outputs/2 - embeddings/embeddings_chunks.h5'\n",
    "NOME_ARQUIVO_EMBEDDINGS_QUESTOES = 'outputs/2 - embeddings/embeddings_questoes.h5'\n",
    "\n",
    "NOME_MODELO_EMB_OPENAI = \"text-embedding-3-large\"\n",
    "\n",
    "# Modelos disponíveis\n",
    "NOME_MODELOS_EMB = [NOME_MODELO_EMB_OPENAI]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb8e58-540f-482b-9993-66a6ac881ced",
   "metadata": {},
   "source": [
    "# 1. Carregar as bases de dados de chunks e de questões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ef43cc4-72bd-40db-abb8-c912b207d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "chunks_pesquisa = load_jsonl('inputs/chunks_pesquisa.jsonl')\n",
    "questoes = load_jsonl('inputs/questoes.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b9e83-cc42-4e77-891a-ab00f8c95500",
   "metadata": {},
   "source": [
    "# 2. Pesquisas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42005d3f-eca3-4467-a677-e0870660e816",
   "metadata": {},
   "source": [
    "Dicionários onde serão salvos os resultados das pesquisas. São dois dicionários, um para todos os chunks e outro para apenas os chunks de artigos.\n",
    "\n",
    "Ele será populado da seguinte forma:\n",
    "\n",
    "<code>resultados_pesquisas = {\n",
    "   \"id_consulta\": {\n",
    "       \"modelo\": {\n",
    "           urn: [\"\", ..., \"\"],\n",
    "           score: [\"\", ..., \"\"]\n",
    "       }\n",
    "   }\n",
    "}</code>\n",
    "\n",
    "onde modelo é o modelo utilizado (por exemplo, bm25, algum modelo de embeddings etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f82c6311-c247-42a8-aa6c-cb23ab63c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_pesquisas_todos_chunks = {}\n",
    "resultados_pesquisas_apenas_art = {}\n",
    "\n",
    "for q in questoes:\n",
    "    id_questao = q['ID_QUESTAO']\n",
    "    resultados_pesquisas_todos_chunks[id_questao] = {}\n",
    "    resultados_pesquisas_apenas_art[id_questao] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0500f50-1c42-47d8-a415-e9e555387037",
   "metadata": {},
   "source": [
    "## 2.1. Pesquisa léxica - BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81767d6c-2585-487c-ab4b-b460d6b826e6",
   "metadata": {},
   "source": [
    "Recupera os índices invertidos gerados no caderno 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9470c0-ec1b-44d8-9e2a-991bc90005c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índice para todos os chunks\n",
    "iidx_todos_chunks = IndiceInvertido(tokenizador_pt_remove_html)\n",
    "iidx_todos_chunks.from_pickle(NOME_ARQUIVO_INDICE_BM25_TODOS_CHUNKS)\n",
    "\n",
    "# Índice para os chunks apenas de artigos\n",
    "iidx_apenas_art = IndiceInvertido(tokenizador_pt_remove_html)\n",
    "iidx_apenas_art.from_pickle(NOME_ARQUIVO_INDICE_BM25_APENAS_ART)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f87fee-2305-4f49-a3bb-91d150dcebfb",
   "metadata": {},
   "source": [
    "Cria instâncias para os buscadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f684d8a3-7d22-492b-b5cb-deba1bf76338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora instancia um BM25\n",
    "buscador_todos_chunks = BM25(iidx_todos_chunks, k1=0.82, b=0.68, bias_idf=1)\n",
    "buscador_apenas_art = BM25(iidx_apenas_art, k1=0.82, b=0.68, bias_idf=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ab2cc-e4b4-4847-9b5e-94728d385269",
   "metadata": {},
   "source": [
    "Pesquisa todas as queries e salva os resultados nos dicionários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c46d6a83-45a9-440c-8a5f-baaeb4ed0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 700/700 [00:53<00:00, 13.00it/s]\n"
     ]
    }
   ],
   "source": [
    "for q in tqdm(questoes):\n",
    "    id_questao = q['ID_QUESTAO']\n",
    "    questao = q['ENUNCIADO_COM_ALTERNATIVAS']\n",
    "\n",
    "    resultado_query_todos_chunks = buscador_todos_chunks.pesquisar(questao)\n",
    "    urn, score = zip(*resultado_query_todos_chunks[:k])\n",
    "    resultados_pesquisas_todos_chunks[id_questao]['bm25'] = { 'urn': urn, 'score': score }\n",
    "    \n",
    "    resultado_query_apenas_art = buscador_apenas_art.pesquisar(questao)\n",
    "    urn, score = zip(*resultado_query_apenas_art[:k])\n",
    "    resultados_pesquisas_apenas_art[id_questao]['bm25'] = { 'urn': urn, 'score': score }    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa21f01-7e0b-4be4-a899-bdda21b451c1",
   "metadata": {},
   "source": [
    "## 2.2 Pesquisas semânticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485fde7-fdb0-4655-bacb-71dc142784cf",
   "metadata": {},
   "source": [
    "### 2.2.1 Funções para carregar e normalizar embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1ce93-7aac-4485-8158-fd053c92186b",
   "metadata": {},
   "source": [
    "Funções genéricas para carregar e normalizar os embeddings do arquivo h5.\n",
    "\n",
    "A ideia é que há apenas dois arquivos, um para os embeddings das questões e outro para os embeddings dos chunks.\n",
    "\n",
    "\r\n",
    "O:.: Os embeddings da OpenAI já são normalizados (mas há uma perda na conversão de f32 pra f16 para salvar no H5), nem precisaria disso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff903ed-86d1-4f36-984a-c935eeebbbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_embeddings_do_arquivo(arquivo, coluna_id, nome_modelo):\n",
    "    with h5py.File(arquivo, \"r\") as f:\n",
    "        ids = f[coluna_id][:].astype(str)\n",
    "        emb = f[nome_modelo][:].astype(np.float32)\n",
    "\n",
    "    return ids, emb\n",
    "\n",
    "def normalizar_embeddings(x):\n",
    "    faiss.normalize_L2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00dfe3-17e2-4498-985d-a6e143bb1a7e",
   "metadata": {},
   "source": [
    "### 2.2.2 Funções para filtrar chunks (todos os chunks ou apenas representando artigos completos e jurisprudências)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eed477-0fcf-42d9-ae36-540a84337b3c",
   "metadata": {},
   "source": [
    "Grupo de funções para filtrar os embeddings dos chunks.\n",
    "\n",
    "Os embeddings foram gerados para todos os chunks. Como serão feitos dois grupos de testes (com todos os chunks e com chunks apenas de artigos), é necessário fazer essa filtragem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fada79de-5953-4b6e-bcdb-278b27b65591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A ideia desse grupo de funções é, uma vez carregado todos os embeddings, filtrar apenas aqueles\n",
    "# que batem com a máscara. Será usado para indexar apenas os embeddings que se referem a artigos completos\n",
    "urns_chunks_pesquisa_apenas_art = [c['URN'] for c in chunks_pesquisa if c['TIPO'] == 'ART' or c['TIPO'] == 'JUR']\n",
    "def filtro_apenas_art(urn):\n",
    "    return urn in urns_chunks_pesquisa_apenas_art\n",
    "\n",
    "def filtro_seleciona_tudo(_):\n",
    "    return True\n",
    "    \n",
    "def filtrar_embeddings(urns, emb, func_filtro):\n",
    "    mask = np.array([func_filtro(i) for i in urns], dtype=bool)\n",
    "\n",
    "    return urns[mask], emb[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8525db2-9366-4344-bb25-b33f27e3bd4b",
   "metadata": {},
   "source": [
    "### 2.2.3 Criação de índices FAISS para todos os modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dc7046-0c3a-4807-be06-865d6d6ba5f7",
   "metadata": {},
   "source": [
    "Função para criar um índice FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5bafc86-8ed0-46cf-9c52-d984468435eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_indice_faiss(nome_modelo, filtro_seleciona_urn):\n",
    "    urns, emb = carregar_embeddings_do_arquivo(NOME_ARQUIVO_EMBEDDINGS_CHUNKS, 'urn', nome_modelo)\n",
    "    urns, emb = filtrar_embeddings(urns, emb, filtro_seleciona_urn)\n",
    "    emb = normalizar_embeddings(emb)\n",
    "    dim = emb.shape[1]\n",
    "        \n",
    "    index_faiss = faiss.IndexFlatL2(dim)\n",
    "    index_faiss.add(emb)\n",
    "\n",
    "    return urns, index_faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd941973-b021-4b42-93bb-ef9727d49229",
   "metadata": {},
   "source": [
    "Cria os índices FAISS para cada modelo.\n",
    "\n",
    "Como serão testados mais de um modelo de embeddings, será criado um mapa de índice no seguinte formato:\n",
    "\n",
    "<code>mapa_indice_faiss = {\n",
    "   'nome_modelo_1': {'urn': lista_de_urns, 'indice': indice_faiss),\n",
    "   'nome_modelo_...': {'urn': lista_de_urns, 'indice': indice_faiss),\n",
    "   'nome_modelo_n': {'urn': lista_de_urns, 'indice': indice_faiss)\n",
    "}</code>\n",
    "\n",
    "Obs.: Devido à forma como os embeddings são criados no caderno 2, todas as urns estão na mesma sequência. Então não precisaria de salvar uma lista de urn para cada modelo (poderíamos apenas guardar. Mas vamos deixar dessa forma, pois futuramente a geração de embeddings pode mudar. Além disso, esse índice é criado apenas na memória e essas listas de urns ocupam pouco espaço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4f96161-6703-40de-9ea7-50a22cd87f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n"
     ]
    }
   ],
   "source": [
    "mapa_indice_faiss_todos_chunks = {}\n",
    "mapa_indice_faiss_apenas_art = {}\n",
    "\n",
    "for nome_modelo in tqdm(NOME_MODELOS_EMB):\n",
    "    urns, indice = criar_indice_faiss(nome_modelo, filtro_seleciona_tudo)\n",
    "    mapa_indice_faiss_todos_chunks[nome_modelo] = {'urn': urns, 'indice': indice}\n",
    "\n",
    "    urns, indice = criar_indice_faiss(nome_modelo, filtro_apenas_art)\n",
    "    mapa_indice_faiss_apenas_art[nome_modelo] = {'urn': urns, 'indice': indice}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5527f22b-92ed-4f9e-b285-6b1c6ea0a81a",
   "metadata": {},
   "source": [
    "### 2.2.4 Mapa de embeddings das questões (queries) por modelo e normalização dos embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc1b451-9360-4bea-ac32-f9dc35c5d034",
   "metadata": {},
   "source": [
    "Cria mapa dos embeddings das questões por modelo.\n",
    "\n",
    "Como serão testados mais de um modelo de embeddings, será criado um mapa da seguinte forma:\n",
    "\n",
    "<code>mapa_emb_questao = {\r\n",
    "   'nome_modelo_1':{'id': lista_de_idss,'emb': lista_de_embeddingse),\r\n",
    "   'nome_modelo_...'{'id': lista_de_ids, 'emb': lista_de_embeddings),),\r\n",
    "   'nome_modelo_n{'id': lista_de_ids, 'emb': lista_de_embeddings),ce)\r\n",
    "}\r\n",
    "\r\n",
    "Assim como para o mapa de índice (e urns dos chunks de pesquisa), dbs.: Devido à forma como os embeddings são criados no caderno 2, todids das questõess urns estão na mesma sequência. Então não precisaria de salvar uma lisidde urn para cada uarda, assim como foi feito antes,r. Mas vamos deixar dessa aqui também.spaço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9ad6e66-d186-46a4-8cfd-9a3791fc69ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 25.14it/s]\n"
     ]
    }
   ],
   "source": [
    "mapa_emb_questao = {}\n",
    "\n",
    "for nome_modelo in tqdm(NOME_MODELOS_EMB):\n",
    "    ids_q, emb_q = carregar_embeddings_do_arquivo(NOME_ARQUIVO_EMBEDDINGS_QUESTOES, 'id', nome_modelo)\n",
    "    emb_q = normalizar_embeddings(emb_q)\n",
    "    mapa_emb_questao[nome_modelo] = {'id': ids_q, 'emb': emb_q}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55737c-3cb1-4ca0-bf77-c2e129107e1a",
   "metadata": {},
   "source": [
    "Agora pesquisa todas as queries em todos os modelos e salva nos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5c5cd1-79f1-40cb-9296-a7c3de5863fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 700/700 [00:06<00:00, 110.72it/s]\n"
     ]
    }
   ],
   "source": [
    "total_porcentagem = len(NOME_MODELOS_EMB)*len(questoes)\n",
    "\n",
    "def get_urn_score(lista_urns_indexadas, distancias, indices_retornados):\n",
    "    # As distâncias e os índices retornados são no shape (1, k). Primeiro, transforma tudo em lista de tamanho k:\n",
    "    distancias = list(distancias[0])\n",
    "    indices_retornados = indices_retornados = list(indices_retornados[0])\n",
    "    \n",
    "    urn = []\n",
    "    score = []\n",
    "    \n",
    "    for d, i in zip(distancias, indices_retornados):\n",
    "        urn.append(lista_urns_indexadas[i])\n",
    "        score.append(1 - d/2) # O score dessa forma é a similaridade de cosseno\n",
    "        \n",
    "    return urn, score\n",
    "    \n",
    "with tqdm(total=total_porcentagem) as pbar:\n",
    "    for nome_modelo in NOME_MODELOS_EMB:\n",
    "        ids_q, embs_q = mapa_emb_questao[nome_modelo]['id'], mapa_emb_questao[nome_modelo]['emb']\n",
    "\n",
    "        for idx, id_questao in enumerate(ids_q):\n",
    "            emb_questao = embs_q[idx:idx+1] # shape (1, dim)\n",
    "\n",
    "            # Consulta no índice de todos os chunks\n",
    "            distancias, indices_retornados = mapa_indice_faiss_todos_chunks[nome_modelo]['indice'].search(emb_questao, k)\n",
    "            urn, score = get_urn_score(mapa_indice_faiss_todos_chunks[nome_modelo]['urn'], distancias, indices_retornados)\n",
    "            resultados_pesquisas_todos_chunks[id_questao][nome_modelo] = { 'urn': urn, 'score': score }\n",
    "\n",
    "            # Consulta no índice com apenas os artigos\n",
    "            distancias, indices_retornados = mapa_indice_faiss_apenas_art[nome_modelo]['indice'].search(emb_questao, k)            \n",
    "            urn, score = get_urn_score(mapa_indice_faiss_apenas_art[nome_modelo]['urn'], distancias, indices_retornados)\n",
    "            resultados_pesquisas_apenas_art[id_questao][nome_modelo] = { 'urn': urn, 'score': score }\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a29af-0c91-43f1-aed6-c6d30c86034a",
   "metadata": {},
   "source": [
    "# 3. Teste: resultados por modelo por questão:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93093c-f86f-422b-ab09-a97e4f78e7f7",
   "metadata": {},
   "source": [
    "Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc252855-1a83-4ced-bdda-37060f19de5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Considerando todos os chunks *****\n",
      "\tbm25\n",
      "\t\tURN: tema_stf_483 (164.937)\n",
      "\t\tURN: urn:lex:br:autoridade.nacional.protecao.dados;conselho.diretor:resolucao:2024-07-16;18;anexo.1!cap2_sec2 (113.247)\n",
      "\t\tURN: urn:lex:br:autoridade.nacional.protecao.dados;conselho.diretor:resolucao:2024-07-16;18;anexo.1!art9 (111.581)\n",
      "\t\tURN: urn:lex:br:autoridade.nacional.protecao.dados;conselho.diretor:resolucao:2024-07-16;18;anexo.1!cap2 (109.955)\n",
      "\t\tURN: urn:lex:br:federal:lei:2011-11-18;12527!cap2 (100.357)\n",
      "\ttext-embedding-3-large\n",
      "\t\tURN: tema_stf_483 (0.598)\n",
      "\t\tURN: urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt (0.531)\n",
      "\t\tURN: urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt_inc4 (0.522)\n",
      "\t\tURN: urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt_inc2 (0.518)\n",
      "\t\tURN: urn:lex:br:federal:lei:2018-08-14;13709!art23_cpt_inc1 (0.512)\n",
      "\n",
      "***** Considerando apenas os chunks de artigos *****\n",
      "\tbm25\n",
      "\t\tURN: tema_stf_483 (146.101)\n",
      "\t\tURN: urn:lex:br:autoridade.nacional.protecao.dados;conselho.diretor:resolucao:2024-07-16;18;anexo.1!art9 (104.069)\n",
      "\t\tURN: urn:lex:br:federal:constituicao:1988-10-05;1988!art37 (94.311)\n",
      "\t\tURN: urn:lex:br:federal:lei:1997-07-16;9472!art3 (91.803)\n",
      "\t\tURN: urn:lex:br:federal:constituicao:1988-10-05;1988!art5 (88.583)\n",
      "\ttext-embedding-3-large\n",
      "\t\tURN: tema_stf_483 (0.598)\n",
      "\t\tURN: urn:lex:br:federal:lei:2011-11-18;12527!art34 (0.509)\n",
      "\t\tURN: urn:lex:br:autoridade.nacional.protecao.dados;conselho.diretor:resolucao:2024-07-16;18;anexo.1!art5 (0.505)\n",
      "\t\tURN: urn:lex:br:federal:lei:2018-08-14;13709!art23 (0.505)\n",
      "\t\tURN: urn:lex:br:federal:lei:2018-08-14;13709!art26 (0.495)\n"
     ]
    }
   ],
   "source": [
    "id_questao = '675'\n",
    "mostrar_top_k = 5\n",
    "todos_modelos = resultados_pesquisas_apenas_art['1'].keys()\n",
    "\n",
    "print('***** Considerando todos os chunks *****')\n",
    "for modelo in todos_modelos:\n",
    "    print(f'\\t{modelo}')\n",
    "    top_k_urn = resultados_pesquisas_todos_chunks[id_questao][modelo]['urn'][:mostrar_top_k]\n",
    "    top_k_score = resultados_pesquisas_todos_chunks[id_questao][modelo]['score'][:mostrar_top_k]\n",
    "    for urn, score in zip(top_k_urn, top_k_score):\n",
    "        print(f'\\t\\tURN: {urn} ({score:.3f})')\n",
    "\n",
    "print('\\n***** Considerando apenas os chunks de artigos *****')\n",
    "for modelo in todos_modelos:\n",
    "    print(f'\\t{modelo}')\n",
    "    top_k_urn = resultados_pesquisas_apenas_art[id_questao][modelo]['urn'][:mostrar_top_k]\n",
    "    top_k_score = resultados_pesquisas_apenas_art[id_questao][modelo]['score'][:mostrar_top_k]\n",
    "    for urn, score in zip(top_k_urn, top_k_score):\n",
    "        print(f'\\t\\tURN: {urn} ({score:.3f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c24f16-34b0-4909-8e66-3da5bf568e7e",
   "metadata": {},
   "source": [
    "# 4. Salva os resultados em arquivos pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a298865-f07d-46e2-bbfe-2f1f6dbcbf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(NOME_ARQUIVO_RESULTADOS_PESQUISAS_TODOS_CHUNKS, 'wb') as f:\n",
    "    pickle.dump(resultados_pesquisas_todos_chunks, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with gzip.open(NOME_ARQUIVO_RESULTADOS_PESQUISAS_APENAS_ART, 'wb') as f:\n",
    "    pickle.dump(resultados_pesquisas_apenas_art, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0e5bfc9-e533-4c22-8cf0-f37eeaf3552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para abrir os arquivos depois:\n",
    "def load_pickle_gzip(path):\n",
    "    import pickle\n",
    "    import gzip\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# temp = load_pickle_gzip(NOME_ARQUIVO_RESULTADOS_PESQUISAS_TODOS_CHUNKS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
